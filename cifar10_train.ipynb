{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x231cc2bfb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\n",
      "\n",
      "(128, 4, 32, 32, 3)\n",
      "conv1\n",
      "\t(128, 4, 32, 32, 3) --> (128, 4, 32, 32, 64)\n",
      "pool1\n",
      "\t(128, 4, 32, 32, 64) --> (128, 4, 16, 16, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cindy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm1\n",
      "\t(128, 4, 16, 16, 64) --> (128, 4, 16, 16, 64)\n",
      "conv2\n",
      "\t(128, 4, 16, 16, 64) --> (128, 4, 16, 16, 64)\n",
      "norm2\n",
      "\t(128, 4, 16, 16, 64) --> (128, 4, 16, 16, 64)\n",
      "pool2\n",
      "\t(128, 4, 16, 16, 64) --> (128, 4, 8, 8, 64)\n",
      "conv2\n",
      "\t(128, 4, 8, 8, 64) --> (128, 4, 8, 8, 64)\n",
      "norm3\n",
      "\t(128, 4, 8, 8, 64) --> (128, 4, 8, 8, 64)\n",
      "pool3\n",
      "\t(128, 4, 8, 8, 64) --> (128, 4, 4, 4, 64)\n",
      "conv4\n",
      "\t(128, 4, 4, 4, 64) --> (128, 4, 4, 4, 64)\n",
      "norm3\n",
      "\t(128, 4, 4, 4, 64) --> (128, 4, 4, 4, 64)\n",
      "pool4\n",
      "\t(128, 4, 4, 4, 64) --> (128, 4, 2, 2, 64)\n",
      "conv5\n",
      "\t(128, 4, 2, 2, 64) --> (128, 4, 2, 2, 64)\n",
      "norm5\n",
      "\t(128, 4, 2, 2, 64) --> (128, 4, 4, 4, 64)\n",
      "pool5\n",
      "\t(128, 4, 4, 4, 64) --> (128, 4, 2, 2, 64)\n",
      "INFO:tensorflow:Summary name conv1/weight_loss (raw) is illegal; using conv1/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv2/weight_loss (raw) is illegal; using conv2/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv3/weight_loss (raw) is illegal; using conv3/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv4/weight_loss (raw) is illegal; using conv4/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv5/weight_loss (raw) is illegal; using conv5/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name softmax_linear/weight_loss (raw) is illegal; using softmax_linear/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n",
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/cifar10_train\\model.ckpt.\n",
      "128\n",
      "2018-06-02 22:27:15.808143: step 0, loss = 3.46 (120.8 examples/sec; 1.060 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:17.957552: step 10, loss = 3.23 (595.2 examples/sec; 0.215 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:20.002319: step 20, loss = 3.09 (626.0 examples/sec; 0.204 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:22.030825: step 30, loss = 3.03 (631.0 examples/sec; 0.203 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:24.092928: step 40, loss = 2.99 (620.7 examples/sec; 0.206 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:26.159680: step 50, loss = 2.98 (619.3 examples/sec; 0.207 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:28.232287: step 60, loss = 3.04 (617.9 examples/sec; 0.207 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:30.333982: step 70, loss = 2.90 (609.0 examples/sec; 0.210 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:32.401739: step 80, loss = 2.71 (618.7 examples/sec; 0.207 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:34.484661: step 90, loss = 2.58 (614.5 examples/sec; 0.208 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.23129\n",
      "128\n",
      "2018-06-02 22:27:37.875656: step 100, loss = 2.79 (378.5 examples/sec; 0.338 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:39.951951: step 110, loss = 2.50 (613.8 examples/sec; 0.209 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:42.005330: step 120, loss = 2.57 (623.4 examples/sec; 0.205 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:44.063803: step 130, loss = 2.46 (621.8 examples/sec; 0.206 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:46.171634: step 140, loss = 2.75 (607.5 examples/sec; 0.211 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:48.301695: step 150, loss = 2.48 (600.6 examples/sec; 0.213 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:50.443890: step 160, loss = 2.40 (597.5 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:52.603041: step 170, loss = 2.56 (592.8 examples/sec; 0.216 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:54.739313: step 180, loss = 2.43 (599.2 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:56.928675: step 190, loss = 2.39 (584.6 examples/sec; 0.219 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.47347\n",
      "128\n",
      "2018-06-02 22:28:00.204759: step 200, loss = 2.53 (390.7 examples/sec; 0.328 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:02.311251: step 210, loss = 2.42 (607.6 examples/sec; 0.211 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:04.464075: step 220, loss = 2.26 (594.6 examples/sec; 0.215 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:06.669898: step 230, loss = 2.32 (580.5 examples/sec; 0.220 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:08.857058: step 240, loss = 2.35 (585.2 examples/sec; 0.219 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:11.114425: step 250, loss = 2.28 (567.0 examples/sec; 0.226 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:13.368016: step 260, loss = 2.25 (567.7 examples/sec; 0.225 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:15.631142: step 270, loss = 2.23 (565.8 examples/sec; 0.226 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:18.034117: step 280, loss = 2.40 (532.9 examples/sec; 0.240 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:20.444937: step 290, loss = 2.20 (530.5 examples/sec; 0.241 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.15301\n",
      "128\n",
      "2018-06-02 22:28:24.293631: step 300, loss = 2.30 (332.6 examples/sec; 0.385 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:26.425865: step 310, loss = 2.13 (600.3 examples/sec; 0.213 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:28.544199: step 320, loss = 2.05 (604.2 examples/sec; 0.212 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:30.662762: step 330, loss = 2.05 (604.2 examples/sec; 0.212 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:32.798879: step 340, loss = 2.08 (599.2 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:34.999105: step 350, loss = 2.21 (581.8 examples/sec; 0.220 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:37.108189: step 360, loss = 2.03 (606.9 examples/sec; 0.211 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:39.290381: step 370, loss = 1.89 (586.6 examples/sec; 0.218 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:41.451346: step 380, loss = 2.06 (592.3 examples/sec; 0.216 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:43.655222: step 390, loss = 2.13 (580.8 examples/sec; 0.220 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.29416\n",
      "128\n",
      "2018-06-02 22:28:47.619996: step 400, loss = 1.97 (322.8 examples/sec; 0.396 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:49.827953: step 410, loss = 2.13 (579.7 examples/sec; 0.221 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:52.053477: step 420, loss = 2.08 (575.1 examples/sec; 0.223 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:54.241169: step 430, loss = 1.97 (585.1 examples/sec; 0.219 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:56.365371: step 440, loss = 1.93 (602.6 examples/sec; 0.212 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:58.530885: step 450, loss = 1.87 (591.1 examples/sec; 0.217 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:00.697839: step 460, loss = 1.98 (590.7 examples/sec; 0.217 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:02.834422: step 470, loss = 1.84 (599.1 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:04.976900: step 480, loss = 1.75 (597.4 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:07.125591: step 490, loss = 2.17 (595.7 examples/sec; 0.215 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.35739\n",
      "128\n",
      "2018-06-02 22:29:10.549818: step 500, loss = 1.83 (375.0 examples/sec; 0.341 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:13.479008: step 510, loss = 1.84 (437.7 examples/sec; 0.292 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:16.535787: step 520, loss = 1.85 (416.6 examples/sec; 0.307 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:19.609005: step 530, loss = 1.81 (416.6 examples/sec; 0.307 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:22.667748: step 540, loss = 2.04 (418.5 examples/sec; 0.306 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:25.683165: step 550, loss = 1.83 (424.5 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:28.721042: step 560, loss = 1.87 (421.2 examples/sec; 0.304 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:31.737976: step 570, loss = 1.95 (424.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:34.760891: step 580, loss = 1.72 (423.3 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:37.752313: step 590, loss = 1.81 (427.9 examples/sec; 0.299 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.15742\n",
      "128\n",
      "2018-06-02 22:29:42.197050: step 600, loss = 1.89 (288.0 examples/sec; 0.444 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:45.186025: step 610, loss = 1.74 (428.2 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:48.195980: step 620, loss = 1.60 (425.4 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:51.211912: step 630, loss = 1.84 (424.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:54.220900: step 640, loss = 1.81 (425.4 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:57.203889: step 650, loss = 1.67 (429.0 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:00.182948: step 660, loss = 1.68 (429.8 examples/sec; 0.298 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "2018-06-02 22:30:03.159963: step 670, loss = 1.75 (430.0 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:06.138998: step 680, loss = 1.49 (429.7 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:09.120025: step 690, loss = 1.82 (429.2 examples/sec; 0.298 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.20175\n",
      "128\n",
      "2018-06-02 22:30:13.444924: step 700, loss = 1.72 (296.7 examples/sec; 0.431 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:16.439915: step 710, loss = 1.76 (425.8 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:19.427968: step 720, loss = 1.61 (428.4 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:22.443904: step 730, loss = 1.77 (424.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:25.441856: step 740, loss = 1.56 (427.0 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:28.441834: step 750, loss = 1.58 (426.7 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:31.463579: step 760, loss = 1.43 (423.6 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:34.463497: step 770, loss = 1.51 (426.7 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:37.495059: step 780, loss = 1.69 (422.4 examples/sec; 0.303 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:40.484793: step 790, loss = 1.51 (428.0 examples/sec; 0.299 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.06646\n",
      "128\n",
      "2018-06-02 22:30:46.063617: step 800, loss = 1.50 (229.4 examples/sec; 0.558 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:49.051273: step 810, loss = 1.34 (428.4 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:52.037852: step 820, loss = 1.59 (428.6 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:55.060948: step 830, loss = 1.42 (423.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:58.061369: step 840, loss = 1.59 (426.6 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:01.042941: step 850, loss = 1.51 (429.3 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:04.044887: step 860, loss = 1.63 (426.4 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:07.035090: step 870, loss = 1.43 (428.1 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:10.033271: step 880, loss = 1.45 (429.2 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:13.006619: step 890, loss = 1.53 (428.2 examples/sec; 0.299 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.20328\n",
      "128\n",
      "2018-06-02 22:31:17.284513: step 900, loss = 1.72 (299.4 examples/sec; 0.427 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:20.294110: step 910, loss = 1.67 (425.0 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:23.349042: step 920, loss = 1.44 (418.9 examples/sec; 0.306 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:26.346840: step 930, loss = 1.23 (427.0 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:29.341587: step 940, loss = 1.31 (427.4 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:32.332013: step 950, loss = 1.44 (428.0 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:35.304723: step 960, loss = 1.50 (430.6 examples/sec; 0.297 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:38.326900: step 970, loss = 1.56 (423.5 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:41.307943: step 980, loss = 1.47 (429.4 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:44.300473: step 990, loss = 1.52 (427.9 examples/sec; 0.299 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.19014\n",
      "128\n",
      "2018-06-02 22:31:48.618741: step 1000, loss = 1.42 (297.0 examples/sec; 0.431 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:51.601892: step 1010, loss = 1.45 (427.6 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:54.610587: step 1020, loss = 1.49 (425.4 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:31:57.603662: step 1030, loss = 1.38 (427.7 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:00.583944: step 1040, loss = 1.40 (429.5 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:03.558555: step 1050, loss = 1.25 (430.3 examples/sec; 0.297 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:06.548367: step 1060, loss = 1.40 (428.1 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:09.525309: step 1070, loss = 1.38 (430.0 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:12.525032: step 1080, loss = 1.25 (426.8 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:15.493901: step 1090, loss = 1.33 (431.0 examples/sec; 0.297 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.23566\n",
      "128\n",
      "2018-06-02 22:32:19.513610: step 1100, loss = 1.15 (318.4 examples/sec; 0.402 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:22.526604: step 1110, loss = 1.16 (424.8 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:25.502662: step 1120, loss = 1.20 (430.1 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:28.491012: step 1130, loss = 1.16 (428.3 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:31.463129: step 1140, loss = 1.39 (430.7 examples/sec; 0.297 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:34.473004: step 1150, loss = 1.24 (425.3 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:37.522295: step 1160, loss = 1.36 (420.2 examples/sec; 0.305 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:40.578107: step 1170, loss = 1.31 (418.6 examples/sec; 0.306 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:43.591050: step 1180, loss = 1.29 (424.7 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:46.635933: step 1190, loss = 1.36 (420.5 examples/sec; 0.304 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.19554\n",
      "128\n",
      "2018-06-02 22:32:50.807791: step 1200, loss = 1.19 (306.7 examples/sec; 0.417 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:53.827678: step 1210, loss = 1.38 (423.9 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:56.846605: step 1220, loss = 1.33 (424.0 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:32:59.858552: step 1230, loss = 1.21 (425.0 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:02.894438: step 1240, loss = 1.24 (421.8 examples/sec; 0.303 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:05.919253: step 1250, loss = 1.18 (423.0 examples/sec; 0.303 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:08.921535: step 1260, loss = 1.24 (426.5 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:11.933322: step 1270, loss = 1.24 (424.9 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:14.956108: step 1280, loss = 1.27 (423.5 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:17.968059: step 1290, loss = 1.15 (425.0 examples/sec; 0.301 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.19761\n",
      "128\n",
      "2018-06-02 22:33:22.095959: step 1300, loss = 1.19 (310.1 examples/sec; 0.413 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:25.106250: step 1310, loss = 1.24 (425.2 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:28.130215: step 1320, loss = 1.27 (425.5 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:31.148547: step 1330, loss = 1.25 (421.9 examples/sec; 0.303 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:34.168428: step 1340, loss = 1.17 (424.0 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:37.186784: step 1350, loss = 1.37 (424.1 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:40.182728: step 1360, loss = 1.22 (427.2 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:43.182571: step 1370, loss = 1.16 (426.6 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:46.197509: step 1380, loss = 1.29 (424.7 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:49.192500: step 1390, loss = 1.26 (427.2 examples/sec; 0.300 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.16732\n",
      "128\n",
      "2018-06-02 22:33:53.655606: step 1400, loss = 1.18 (286.8 examples/sec; 0.446 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:56.626784: step 1410, loss = 1.21 (430.8 examples/sec; 0.297 sec/batch)\n",
      "128\n",
      "2018-06-02 22:33:59.627753: step 1420, loss = 1.10 (426.7 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:02.675567: step 1430, loss = 1.07 (419.8 examples/sec; 0.305 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:05.702866: step 1440, loss = 1.18 (422.8 examples/sec; 0.303 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:08.716609: step 1450, loss = 1.19 (424.7 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:11.747503: step 1460, loss = 1.31 (422.5 examples/sec; 0.303 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:14.753465: step 1470, loss = 1.08 (425.7 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:17.738482: step 1480, loss = 1.00 (428.8 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:20.745475: step 1490, loss = 1.06 (425.7 examples/sec; 0.301 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.17611\n",
      "128\n",
      "2018-06-02 22:34:25.149666: step 1500, loss = 1.06 (291.0 examples/sec; 0.440 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:28.212475: step 1510, loss = 1.12 (417.1 examples/sec; 0.307 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "2018-06-02 22:34:31.229408: step 1520, loss = 1.05 (424.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:34.254321: step 1530, loss = 1.08 (423.0 examples/sec; 0.303 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:37.241553: step 1540, loss = 0.98 (428.5 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:40.222301: step 1550, loss = 1.19 (429.6 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:43.204446: step 1560, loss = 1.23 (429.1 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:46.197484: step 1570, loss = 1.10 (427.8 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:49.187495: step 1580, loss = 1.03 (428.0 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:52.189763: step 1590, loss = 1.05 (426.5 examples/sec; 0.300 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.20767\n",
      "128\n",
      "2018-06-02 22:34:56.345925: step 1600, loss = 1.06 (308.3 examples/sec; 0.415 sec/batch)\n",
      "128\n",
      "2018-06-02 22:34:59.338869: step 1610, loss = 1.00 (427.0 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:02.339330: step 1620, loss = 1.03 (426.5 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:05.335318: step 1630, loss = 1.05 (427.2 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:08.321334: step 1640, loss = 0.98 (428.7 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:11.318320: step 1650, loss = 1.03 (427.1 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:14.338276: step 1660, loss = 1.11 (423.8 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:17.339225: step 1670, loss = 1.09 (426.7 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:20.344189: step 1680, loss = 0.94 (425.8 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:23.373099: step 1690, loss = 0.94 (422.6 examples/sec; 0.303 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.20155\n",
      "128\n",
      "2018-06-02 22:35:27.562884: step 1700, loss = 1.03 (306.0 examples/sec; 0.418 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:30.563857: step 1710, loss = 1.01 (425.5 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:33.568822: step 1720, loss = 1.06 (426.1 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:36.569799: step 1730, loss = 0.98 (426.4 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:39.556848: step 1740, loss = 0.99 (428.5 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:42.559789: step 1750, loss = 1.30 (426.2 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:45.561794: step 1760, loss = 1.06 (426.4 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:48.576691: step 1770, loss = 1.02 (424.6 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:51.579835: step 1780, loss = 1.09 (426.2 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:35:54.582978: step 1790, loss = 1.02 (426.2 examples/sec; 0.300 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.20711\n",
      "128\n",
      "2018-06-02 22:35:58.738011: step 1800, loss = 0.94 (308.7 examples/sec; 0.415 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:01.745901: step 1810, loss = 0.93 (424.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:04.747995: step 1820, loss = 0.84 (426.4 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:07.759810: step 1830, loss = 0.96 (425.0 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:10.765007: step 1840, loss = 0.95 (425.9 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:13.760715: step 1850, loss = 0.95 (427.3 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:16.731818: step 1860, loss = 0.98 (430.8 examples/sec; 0.297 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:19.745264: step 1870, loss = 1.01 (424.8 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:22.751565: step 1880, loss = 0.75 (425.8 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:25.769387: step 1890, loss = 0.88 (424.1 examples/sec; 0.302 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.20473\n",
      "128\n",
      "2018-06-02 22:36:29.942559: step 1900, loss = 0.90 (307.4 examples/sec; 0.416 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:32.938922: step 1910, loss = 0.95 (425.9 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:35.918053: step 1920, loss = 1.31 (429.7 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:38.960561: step 1930, loss = 0.86 (420.7 examples/sec; 0.304 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:41.957120: step 1940, loss = 1.06 (427.3 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:44.947172: step 1950, loss = 1.05 (427.9 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:47.926160: step 1960, loss = 0.83 (429.7 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:50.931462: step 1970, loss = 0.90 (425.9 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:53.950026: step 1980, loss = 0.77 (424.0 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:36:56.955787: step 1990, loss = 0.86 (425.8 examples/sec; 0.301 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.18788\n",
      "128\n",
      "2018-06-02 22:37:01.309373: step 2000, loss = 1.02 (294.6 examples/sec; 0.435 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:04.320731: step 2010, loss = 0.87 (424.1 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:07.371483: step 2020, loss = 1.00 (419.6 examples/sec; 0.305 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:10.442090: step 2030, loss = 0.90 (416.9 examples/sec; 0.307 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:13.445631: step 2040, loss = 1.00 (426.2 examples/sec; 0.300 sec/batch)\n",
      "INFO:tensorflow:Saving checkpoints for 2044 into /tmp/cifar10_train\\model.ckpt.\n",
      "128\n",
      "2018-06-02 22:37:18.180255: step 2050, loss = 1.07 (270.3 examples/sec; 0.474 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:21.179721: step 2060, loss = 0.98 (426.7 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:24.144456: step 2070, loss = 1.07 (431.7 examples/sec; 0.296 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:27.153716: step 2080, loss = 0.88 (425.4 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:30.174912: step 2090, loss = 1.00 (423.7 examples/sec; 0.302 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.04332\n",
      "128\n",
      "2018-06-02 22:37:34.161499: step 2100, loss = 1.04 (321.1 examples/sec; 0.399 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:37.169811: step 2110, loss = 0.99 (425.6 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:40.180223: step 2120, loss = 0.98 (425.0 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:43.182364: step 2130, loss = 0.96 (426.4 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:46.172071: step 2140, loss = 1.02 (428.1 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:49.142656: step 2150, loss = 0.78 (430.9 examples/sec; 0.297 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:52.124199: step 2160, loss = 0.84 (429.3 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:55.124382: step 2170, loss = 1.02 (426.6 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:37:58.108169: step 2180, loss = 0.80 (429.0 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:01.080521: step 2190, loss = 0.95 (430.8 examples/sec; 0.297 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.23793\n",
      "128\n",
      "2018-06-02 22:38:05.043479: step 2200, loss = 0.89 (322.9 examples/sec; 0.396 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:08.039070: step 2210, loss = 0.94 (427.3 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:11.058160: step 2220, loss = 0.68 (424.0 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:14.039495: step 2230, loss = 0.91 (429.3 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:17.029076: step 2240, loss = 0.85 (428.2 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:20.021539: step 2250, loss = 0.98 (427.7 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:23.013726: step 2260, loss = 0.78 (427.8 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:26.001282: step 2270, loss = 0.92 (428.4 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:28.973404: step 2280, loss = 0.93 (430.8 examples/sec; 0.297 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:31.957317: step 2290, loss = 0.77 (428.8 examples/sec; 0.298 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.22443\n",
      "128\n",
      "2018-06-02 22:38:36.076305: step 2300, loss = 0.90 (311.4 examples/sec; 0.411 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:39.080296: step 2310, loss = 0.83 (425.0 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:42.065024: step 2320, loss = 0.82 (428.7 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:45.041233: step 2330, loss = 0.89 (430.2 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:48.029738: step 2340, loss = 0.84 (428.3 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:51.008349: step 2350, loss = 0.72 (429.7 examples/sec; 0.298 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "2018-06-02 22:38:54.002234: step 2360, loss = 1.21 (427.4 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:56.992108: step 2370, loss = 0.83 (428.1 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:38:59.977477: step 2380, loss = 0.92 (428.9 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:02.953628: step 2390, loss = 0.77 (430.1 examples/sec; 0.298 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.21212\n",
      "128\n",
      "2018-06-02 22:39:07.204426: step 2400, loss = 0.78 (302.0 examples/sec; 0.424 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:10.185588: step 2410, loss = 0.78 (427.4 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:13.158135: step 2420, loss = 0.82 (430.6 examples/sec; 0.297 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:16.146629: step 2430, loss = 0.80 (428.3 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:19.121903: step 2440, loss = 0.67 (430.2 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:22.140534: step 2450, loss = 0.68 (424.2 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:25.157271: step 2460, loss = 0.89 (424.2 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:28.121929: step 2470, loss = 0.89 (431.8 examples/sec; 0.296 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:31.138185: step 2480, loss = 0.75 (424.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:34.096495: step 2490, loss = 0.80 (432.7 examples/sec; 0.296 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.22165\n",
      "128\n",
      "2018-06-02 22:39:38.237426: step 2500, loss = 0.97 (309.1 examples/sec; 0.414 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:41.229425: step 2510, loss = 0.75 (427.8 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:44.324429: step 2520, loss = 0.87 (413.7 examples/sec; 0.309 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:47.341741: step 2530, loss = 0.76 (424.1 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:50.340722: step 2540, loss = 0.75 (426.8 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:53.344452: step 2550, loss = 0.80 (426.1 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:56.337274: step 2560, loss = 0.80 (427.7 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:39:59.319636: step 2570, loss = 0.90 (429.2 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:40:02.315162: step 2580, loss = 0.79 (427.3 examples/sec; 0.300 sec/batch)\n",
      "128\n",
      "2018-06-02 22:40:05.294932: step 2590, loss = 0.79 (429.6 examples/sec; 0.298 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.20134\n",
      "128\n",
      "2018-06-02 22:40:09.473388: step 2600, loss = 0.68 (306.8 examples/sec; 0.417 sec/batch)\n",
      "128\n",
      "2018-06-02 22:40:12.448967: step 2610, loss = 0.68 (429.2 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:40:15.428392: step 2620, loss = 0.88 (429.6 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:40:18.410648: step 2630, loss = 0.81 (429.3 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:40:21.391068: step 2640, loss = 0.75 (429.3 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:40:24.372588: step 2650, loss = 0.60 (429.3 examples/sec; 0.298 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A binary to train CIFAR-10 using a single GPU.\n",
    "Accuracy:\n",
    "cifar10_train.py achieves ~86% accuracy after 100K steps (256 epochs of\n",
    "data) as judged by cifar10_eval.py.\n",
    "Speed: With batch_size 128.\n",
    "System        | Step Time (sec/batch)  |     Accuracy\n",
    "------------------------------------------------------------------\n",
    "1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n",
    "1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n",
    "Usage:\n",
    "Please see the tutorial and website for how to download the CIFAR-10\n",
    "data set, compile the program and train the model.\n",
    "http://tensorflow.org/tutorials/deep_cnn/\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('log_frequency', 10,\n",
    "                            \"\"\"How often to log results to the console.\"\"\")\n",
    "\n",
    "\n",
    "def train():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n",
    "    # GPU and resulting in a slow down.\n",
    "    with tf.device('/cpu:0'):\n",
    "      images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.b\n",
    "    logits = cifar10.inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = cifar10.loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = cifar10.train(loss, global_step)\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "      \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "      def begin(self):\n",
    "        self._step = -1\n",
    "        self._start_time = time.time()\n",
    "\n",
    "      def before_run(self, run_context):\n",
    "        self._step += 1\n",
    "        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "      def after_run(self, run_context, run_values):\n",
    "        if self._step % FLAGS.log_frequency == 0:\n",
    "          current_time = time.time()\n",
    "          duration = current_time - self._start_time\n",
    "          self._start_time = current_time\n",
    "\n",
    "          loss_value = run_values.results\n",
    "          print(FLAGS.batch_size)\n",
    "          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n",
    "          sec_per_batch = float(duration / FLAGS.log_frequency)\n",
    "\n",
    "          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                        'sec/batch)')\n",
    "          print (format_str % (datetime.now(), self._step, loss_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "\n",
    "    with tf.train.MonitoredTrainingSession(\n",
    "        checkpoint_dir=FLAGS.train_dir,\n",
    "        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n",
    "               tf.train.NanTensorHook(loss),\n",
    "               _LoggerHook()],\n",
    "        config=tf.ConfigProto(\n",
    "            log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n",
    "      while not mon_sess.should_stop():\n",
    "        mon_sess.run(train_op)\n",
    "\n",
    "\n",
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "  cifar10.maybe_download_and_extract()\n",
    "  if tf.gfile.Exists(FLAGS.train_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "  train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
