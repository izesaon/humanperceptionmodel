{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x231cc2bfb70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filling queue with 20000 CIFAR images before starting to train. This will take a few minutes.INFO:tensorflow:Error reported to Coordinator: <class 'tensorflow.python.framework.errors_impl.CancelledError'>, Enqueue operation was cancelled\n",
      "\t [[Node: input_producer/input_producer_EnqueueMany = QueueEnqueueManyV2[Tcomponents=[DT_STRING], timeout_ms=-1, _device=\"/job:localhost/replica:0/task:0/cpu:0\"](input_producer, input_producer/RandomShuffle)]]\n",
      "\n",
      "(128, 4, 32, 32, 3)\n",
      "conv1\n",
      "\t(128, 4, 32, 32, 3) --> (128, 4, 32, 32, 64)\n",
      "pool1\n",
      "\t(128, 4, 32, 32, 64) --> (128, 4, 16, 16, 64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cindy\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm1\n",
      "\t(128, 4, 16, 16, 64) --> (128, 4, 16, 16, 64)\n",
      "conv2\n",
      "\t(128, 4, 16, 16, 64) --> (128, 4, 16, 16, 64)\n",
      "norm2\n",
      "\t(128, 4, 16, 16, 64) --> (128, 4, 16, 16, 64)\n",
      "pool2\n",
      "\t(128, 4, 16, 16, 64) --> (128, 4, 8, 8, 64)\n",
      "conv2\n",
      "\t(128, 4, 8, 8, 64) --> (128, 4, 8, 8, 64)\n",
      "norm3\n",
      "\t(128, 4, 8, 8, 64) --> (128, 4, 8, 8, 64)\n",
      "pool3\n",
      "\t(128, 4, 8, 8, 64) --> (128, 4, 4, 4, 64)\n",
      "conv4\n",
      "\t(128, 4, 4, 4, 64) --> (128, 4, 4, 4, 64)\n",
      "norm3\n",
      "\t(128, 4, 4, 4, 64) --> (128, 4, 4, 4, 64)\n",
      "pool4\n",
      "\t(128, 4, 4, 4, 64) --> (128, 4, 2, 2, 64)\n",
      "conv5\n",
      "\t(128, 4, 2, 2, 64) --> (128, 4, 2, 2, 64)\n",
      "norm5\n",
      "\t(128, 4, 2, 2, 64) --> (128, 4, 4, 4, 64)\n",
      "pool5\n",
      "\t(128, 4, 4, 4, 64) --> (128, 4, 2, 2, 64)\n",
      "INFO:tensorflow:Summary name conv1/weight_loss (raw) is illegal; using conv1/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv2/weight_loss (raw) is illegal; using conv2/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv3/weight_loss (raw) is illegal; using conv3/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv4/weight_loss (raw) is illegal; using conv4/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name conv5/weight_loss (raw) is illegal; using conv5/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local3/weight_loss (raw) is illegal; using local3/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name local4/weight_loss (raw) is illegal; using local4/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name softmax_linear/weight_loss (raw) is illegal; using softmax_linear/weight_loss__raw_ instead.\n",
      "INFO:tensorflow:Summary name cross_entropy (raw) is illegal; using cross_entropy__raw_ instead.\n",
      "INFO:tensorflow:Summary name total_loss (raw) is illegal; using total_loss__raw_ instead.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Saving checkpoints for 1 into /tmp/cifar10_train\\model.ckpt.\n",
      "128\n",
      "2018-06-02 22:27:15.808143: step 0, loss = 3.46 (120.8 examples/sec; 1.060 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:17.957552: step 10, loss = 3.23 (595.2 examples/sec; 0.215 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:20.002319: step 20, loss = 3.09 (626.0 examples/sec; 0.204 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:22.030825: step 30, loss = 3.03 (631.0 examples/sec; 0.203 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:24.092928: step 40, loss = 2.99 (620.7 examples/sec; 0.206 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:26.159680: step 50, loss = 2.98 (619.3 examples/sec; 0.207 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:28.232287: step 60, loss = 3.04 (617.9 examples/sec; 0.207 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:30.333982: step 70, loss = 2.90 (609.0 examples/sec; 0.210 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:32.401739: step 80, loss = 2.71 (618.7 examples/sec; 0.207 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:34.484661: step 90, loss = 2.58 (614.5 examples/sec; 0.208 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.23129\n",
      "128\n",
      "2018-06-02 22:27:37.875656: step 100, loss = 2.79 (378.5 examples/sec; 0.338 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:39.951951: step 110, loss = 2.50 (613.8 examples/sec; 0.209 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:42.005330: step 120, loss = 2.57 (623.4 examples/sec; 0.205 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:44.063803: step 130, loss = 2.46 (621.8 examples/sec; 0.206 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:46.171634: step 140, loss = 2.75 (607.5 examples/sec; 0.211 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:48.301695: step 150, loss = 2.48 (600.6 examples/sec; 0.213 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:50.443890: step 160, loss = 2.40 (597.5 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:52.603041: step 170, loss = 2.56 (592.8 examples/sec; 0.216 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:54.739313: step 180, loss = 2.43 (599.2 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:27:56.928675: step 190, loss = 2.39 (584.6 examples/sec; 0.219 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.47347\n",
      "128\n",
      "2018-06-02 22:28:00.204759: step 200, loss = 2.53 (390.7 examples/sec; 0.328 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:02.311251: step 210, loss = 2.42 (607.6 examples/sec; 0.211 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:04.464075: step 220, loss = 2.26 (594.6 examples/sec; 0.215 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:06.669898: step 230, loss = 2.32 (580.5 examples/sec; 0.220 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:08.857058: step 240, loss = 2.35 (585.2 examples/sec; 0.219 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:11.114425: step 250, loss = 2.28 (567.0 examples/sec; 0.226 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:13.368016: step 260, loss = 2.25 (567.7 examples/sec; 0.225 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:15.631142: step 270, loss = 2.23 (565.8 examples/sec; 0.226 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:18.034117: step 280, loss = 2.40 (532.9 examples/sec; 0.240 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:20.444937: step 290, loss = 2.20 (530.5 examples/sec; 0.241 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.15301\n",
      "128\n",
      "2018-06-02 22:28:24.293631: step 300, loss = 2.30 (332.6 examples/sec; 0.385 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:26.425865: step 310, loss = 2.13 (600.3 examples/sec; 0.213 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:28.544199: step 320, loss = 2.05 (604.2 examples/sec; 0.212 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:30.662762: step 330, loss = 2.05 (604.2 examples/sec; 0.212 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:32.798879: step 340, loss = 2.08 (599.2 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:34.999105: step 350, loss = 2.21 (581.8 examples/sec; 0.220 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:37.108189: step 360, loss = 2.03 (606.9 examples/sec; 0.211 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:39.290381: step 370, loss = 1.89 (586.6 examples/sec; 0.218 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:41.451346: step 380, loss = 2.06 (592.3 examples/sec; 0.216 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:43.655222: step 390, loss = 2.13 (580.8 examples/sec; 0.220 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.29416\n",
      "128\n",
      "2018-06-02 22:28:47.619996: step 400, loss = 1.97 (322.8 examples/sec; 0.396 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:49.827953: step 410, loss = 2.13 (579.7 examples/sec; 0.221 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:52.053477: step 420, loss = 2.08 (575.1 examples/sec; 0.223 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:54.241169: step 430, loss = 1.97 (585.1 examples/sec; 0.219 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:56.365371: step 440, loss = 1.93 (602.6 examples/sec; 0.212 sec/batch)\n",
      "128\n",
      "2018-06-02 22:28:58.530885: step 450, loss = 1.87 (591.1 examples/sec; 0.217 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:00.697839: step 460, loss = 1.98 (590.7 examples/sec; 0.217 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:02.834422: step 470, loss = 1.84 (599.1 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:04.976900: step 480, loss = 1.75 (597.4 examples/sec; 0.214 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:07.125591: step 490, loss = 2.17 (595.7 examples/sec; 0.215 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 4.35739\n",
      "128\n",
      "2018-06-02 22:29:10.549818: step 500, loss = 1.83 (375.0 examples/sec; 0.341 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:13.479008: step 510, loss = 1.84 (437.7 examples/sec; 0.292 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:16.535787: step 520, loss = 1.85 (416.6 examples/sec; 0.307 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:19.609005: step 530, loss = 1.81 (416.6 examples/sec; 0.307 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:22.667748: step 540, loss = 2.04 (418.5 examples/sec; 0.306 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:25.683165: step 550, loss = 1.83 (424.5 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:28.721042: step 560, loss = 1.87 (421.2 examples/sec; 0.304 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:31.737976: step 570, loss = 1.95 (424.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:34.760891: step 580, loss = 1.72 (423.3 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:37.752313: step 590, loss = 1.81 (427.9 examples/sec; 0.299 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.15742\n",
      "128\n",
      "2018-06-02 22:29:42.197050: step 600, loss = 1.89 (288.0 examples/sec; 0.444 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:45.186025: step 610, loss = 1.74 (428.2 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:48.195980: step 620, loss = 1.60 (425.4 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:51.211912: step 630, loss = 1.84 (424.4 examples/sec; 0.302 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:54.220900: step 640, loss = 1.81 (425.4 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:29:57.203889: step 650, loss = 1.67 (429.0 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:00.182948: step 660, loss = 1.68 (429.8 examples/sec; 0.298 sec/batch)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "2018-06-02 22:30:03.159963: step 670, loss = 1.75 (430.0 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:06.138998: step 680, loss = 1.49 (429.7 examples/sec; 0.298 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:09.120025: step 690, loss = 1.82 (429.2 examples/sec; 0.298 sec/batch)\n",
      "INFO:tensorflow:global_step/sec: 3.20175\n",
      "128\n",
      "2018-06-02 22:30:13.444924: step 700, loss = 1.72 (296.7 examples/sec; 0.431 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:16.439915: step 710, loss = 1.76 (425.8 examples/sec; 0.301 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:19.427968: step 720, loss = 1.61 (428.4 examples/sec; 0.299 sec/batch)\n",
      "128\n",
      "2018-06-02 22:30:22.443904: step 730, loss = 1.77 (424.4 examples/sec; 0.302 sec/batch)\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2015 The TensorFlow Authors. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# ==============================================================================\n",
    "\n",
    "\"\"\"A binary to train CIFAR-10 using a single GPU.\n",
    "Accuracy:\n",
    "cifar10_train.py achieves ~86% accuracy after 100K steps (256 epochs of\n",
    "data) as judged by cifar10_eval.py.\n",
    "Speed: With batch_size 128.\n",
    "System        | Step Time (sec/batch)  |     Accuracy\n",
    "------------------------------------------------------------------\n",
    "1 Tesla K20m  | 0.35-0.60              | ~86% at 60K steps  (5 hours)\n",
    "1 Tesla K40m  | 0.25-0.35              | ~86% at 100K steps (4 hours)\n",
    "Usage:\n",
    "Please see the tutorial and website for how to download the CIFAR-10\n",
    "data set, compile the program and train the model.\n",
    "http://tensorflow.org/tutorials/deep_cnn/\n",
    "\"\"\"\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import cifar10\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "tf.app.flags.DEFINE_string('train_dir', '/tmp/cifar10_train',\n",
    "                           \"\"\"Directory where to write event logs \"\"\"\n",
    "                           \"\"\"and checkpoint.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('max_steps', 1000000,\n",
    "                            \"\"\"Number of batches to run.\"\"\")\n",
    "tf.app.flags.DEFINE_boolean('log_device_placement', False,\n",
    "                            \"\"\"Whether to log device placement.\"\"\")\n",
    "tf.app.flags.DEFINE_integer('log_frequency', 10,\n",
    "                            \"\"\"How often to log results to the console.\"\"\")\n",
    "\n",
    "\n",
    "def train():\n",
    "  \"\"\"Train CIFAR-10 for a number of steps.\"\"\"\n",
    "  with tf.Graph().as_default():\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "\n",
    "    # Get images and labels for CIFAR-10.\n",
    "    # Force input pipeline to CPU:0 to avoid operations sometimes ending up on\n",
    "    # GPU and resulting in a slow down.\n",
    "    with tf.device('/cpu:0'):\n",
    "      images, labels = cifar10.distorted_inputs()\n",
    "\n",
    "    # Build a Graph that computes the logits predictions from the\n",
    "    # inference model.b\n",
    "    logits = cifar10.inference(images)\n",
    "\n",
    "    # Calculate loss.\n",
    "    loss = cifar10.loss(logits, labels)\n",
    "\n",
    "    # Build a Graph that trains the model with one batch of examples and\n",
    "    # updates the model parameters.\n",
    "    train_op = cifar10.train(loss, global_step)\n",
    "\n",
    "    class _LoggerHook(tf.train.SessionRunHook):\n",
    "      \"\"\"Logs loss and runtime.\"\"\"\n",
    "\n",
    "      def begin(self):\n",
    "        self._step = -1\n",
    "        self._start_time = time.time()\n",
    "\n",
    "      def before_run(self, run_context):\n",
    "        self._step += 1\n",
    "        return tf.train.SessionRunArgs(loss)  # Asks for loss value.\n",
    "\n",
    "      def after_run(self, run_context, run_values):\n",
    "        if self._step % FLAGS.log_frequency == 0:\n",
    "          current_time = time.time()\n",
    "          duration = current_time - self._start_time\n",
    "          self._start_time = current_time\n",
    "\n",
    "          loss_value = run_values.results\n",
    "          print(FLAGS.batch_size)\n",
    "          examples_per_sec = FLAGS.log_frequency * FLAGS.batch_size / duration\n",
    "          sec_per_batch = float(duration / FLAGS.log_frequency)\n",
    "\n",
    "          format_str = ('%s: step %d, loss = %.2f (%.1f examples/sec; %.3f '\n",
    "                        'sec/batch)')\n",
    "          print (format_str % (datetime.now(), self._step, loss_value,\n",
    "                               examples_per_sec, sec_per_batch))\n",
    "\n",
    "    with tf.train.MonitoredTrainingSession(\n",
    "        checkpoint_dir=FLAGS.train_dir,\n",
    "        hooks=[tf.train.StopAtStepHook(last_step=FLAGS.max_steps),\n",
    "               tf.train.NanTensorHook(loss),\n",
    "               _LoggerHook()],\n",
    "        config=tf.ConfigProto(\n",
    "            log_device_placement=FLAGS.log_device_placement)) as mon_sess:\n",
    "      while not mon_sess.should_stop():\n",
    "        mon_sess.run(train_op)\n",
    "\n",
    "\n",
    "def main(argv=None):  # pylint: disable=unused-argument\n",
    "  cifar10.maybe_download_and_extract()\n",
    "  if tf.gfile.Exists(FLAGS.train_dir):\n",
    "    tf.gfile.DeleteRecursively(FLAGS.train_dir)\n",
    "  tf.gfile.MakeDirs(FLAGS.train_dir)\n",
    "  train()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  tf.app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
